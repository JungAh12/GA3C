{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from multiprocessing import Process, Queue, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Config import Config\n",
    "from Environment import Environment\n",
    "from Experience import Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessAgent(Process):\n",
    "    def __init__(self, id, prediction_q, training_q, episode_log_q):\n",
    "        super(ProcessAgent, self).__init__()\n",
    "        \n",
    "        self.id = id\n",
    "        self.prediction_q = prediction_q\n",
    "        self.training_q = training_q\n",
    "        self.episode_log_q = episode_log_q\n",
    "        \n",
    "        self.env = Environment()\n",
    "        self.num_actions = self.env.get_num_actions()\n",
    "        self.actions = np.arange(self.num_actions)\n",
    "        \n",
    "        self.discount_factor = Config.DISCOUNT\n",
    "        \n",
    "        self.wait_q = Queue(maxsize=1)\n",
    "        self.exit_flag = Value('i', 0)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _accumulate_reward(experiences, discount_factor, terminal_reward):\n",
    "        reward_sum = terminal_reward\n",
    "        for t in reversed(range(0, len(experiences)-1)):\n",
    "            r = np.clip(experiences[t].reward, Config.REWARD_MIN, Config.RESULTS_MAX)\n",
    "            reward_sum = discount_factor * reward_sum + r\n",
    "            experiences[t].reward = reward_sum\n",
    "        return experiences[:-1]\n",
    "    \n",
    "    def convert_data(self, experiences):\n",
    "        x_ = np.array([exp.state for exp in experiences])\n",
    "        a_ = np.eye(self.num_actions)[np.array([exp.action for exp in experiences])].astype(np.float32)\n",
    "        r_ = np.array([exp.reward for exp in experiences])\n",
    "        return x_, r_, a_\n",
    "    \n",
    "    def predict(self, state):\n",
    "        self.prediction_q.put((self.id, state))\n",
    "        p, v = self.wait_q.get()\n",
    "        return p, v\n",
    "    \n",
    "    def select_action(self, prediction):\n",
    "        if Config.PLAY_MODE:\n",
    "            action = np.argmax(prediction)\n",
    "        else:\n",
    "            action = np.random.choice(self.actions, p = prediction)\n",
    "        return action\n",
    "    \n",
    "    def run_episode(self):\n",
    "        self.env.reset()\n",
    "        done = False\n",
    "        experiences = []\n",
    "        \n",
    "        time_count = 0\n",
    "        reward_sum = 0.0\n",
    "        \n",
    "        while not done:\n",
    "            if self.env.current_state is None:\n",
    "                self.env.step(0)\n",
    "                continue\n",
    "                \n",
    "            prediction, value = self.predict(self.env.current_state)\n",
    "            action = self.select_action(prediction)\n",
    "            reward, done = self.env.step(action)\n",
    "            reward_sum += reward\n",
    "            exp = Experience(self.env.previous_state, action, predictioni, reward, done)\n",
    "            experiences.append(exp)\n",
    "            \n",
    "            if done or time_count == Config.TIME_MAX:\n",
    "                terminal_reward = 0 if done else value\n",
    "                \n",
    "                updated_exps = ProcessAgent._accumulate_reward(experiences, self.discount_factor, terminal_reward)\n",
    "                x_, r_, a_ = self.convert_data(updated_exps)\n",
    "                yield x_, r_, a_, reward_sum\n",
    "                \n",
    "                time_count = 0\n",
    "                \n",
    "                experiences = [experiences[-1]]\n",
    "                reward_sum = 0.0\n",
    "                \n",
    "            time_count += 1\n",
    "            \n",
    "    def run(self):\n",
    "        time.sleep(np.random.rand())\n",
    "        np.random.seed(np.int32(time.time() % 1 * 1000 + self.id * 10))\n",
    "        \n",
    "        while self.exit_flag.value == 0:\n",
    "            total_reward = 0\n",
    "            total_length = 0\n",
    "            for x_, r_, a_, reward_sum in self.run_episode():\n",
    "                total_reward += reward_sum\n",
    "                total_length += len(r_) +1\n",
    "                self.training_q.put((x_,r_,a_))\n",
    "            self.episode_log_q.put((datetime.now(), total_reward, total_length))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ga3c",
   "language": "python",
   "name": "ga3c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
