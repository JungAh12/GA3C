{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yg/miniconda3/envs/ga3c/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/yg/miniconda3/envs/ga3c/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/yg/miniconda3/envs/ga3c/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/yg/miniconda3/envs/ga3c/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/yg/miniconda3/envs/ga3c/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/yg/miniconda3/envs/ga3c/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from Config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkVP:\n",
    "    def __init__(self, device, model_name, num_actions):\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.img_width = Config.IMAGE_WIDTH\n",
    "        self.img_height = Config.IMAGE_HEIGHT\n",
    "        self.img_channels = Config.STACKED_FRAMES\n",
    "        \n",
    "        self.learning_rate = Config.LEARNING_RATE_START\n",
    "        self.beta = Config.BETA_START\n",
    "        self.log_epsilon = Config.LOG_EPSILON\n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default() as g:\n",
    "            with tf.device(self.device):\n",
    "                self._create_graph()\n",
    "                \n",
    "                self.sess = tf.Session(\n",
    "                    graph = self.graph,\n",
    "                    config = tf.ConfigProto(\n",
    "                        allow_soft_placement = True,\n",
    "                        log_device_placement = False,\n",
    "                        gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "                    )\n",
    "                )\n",
    "                self.sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "                if Config.TENSORBOARD: self._create_tensor_board()\n",
    "                if Config.LOAD_CHECKPOINT or Config.SAVE_MODELS:\n",
    "                    vars = tf.global_variables()\n",
    "                    self.saver = tf.train.Saver({var.name:var for var in vars}, max_to_keep=0)\n",
    "                    \n",
    "    def _create_graph(self):\n",
    "        self.x = tf.placeholder(\n",
    "            tf.float32, [None, self.img_height, self.img_width, self.img_channels], name='X')\n",
    "        self.y_r = tf.placeholder(tf.float32, [None], name='Yr')\n",
    "        self.var_beta = tf.placeholder(tf.float32, name='beta', shape=[])\n",
    "        self.var_learning_rate = tf.placeholder(tf.float32, name='lr', shape=[])\n",
    "        self.global_step = tf.Variable(0, trainable=False, name='step')\n",
    "        \n",
    "        ####A3C 구현\n",
    "        self.n1 = self.conv2d_layer(self.x, 8, 16, 'conv11', strides=[1,4,4,1])\n",
    "        self.n2 = self.conv2d_layer(self.n1, 4, 32, 'conv12', strides=[1,2,2,1])\n",
    "        self.action_index = tf.placeholder(tf.float32, [None, self.num_actions])\n",
    "        _input = self.n2\n",
    "        \n",
    "        flatten_input_shape = _input.get_shape()\n",
    "        nb_elements = flatten_input_shape[1] * flatten_input_shape[2] * flatten_input_shape[3]\n",
    "        \n",
    "        self.flat = tf.reshape(_input, shape=[-1, nb_elements._value])\n",
    "        print('self.flat.shape = ',self.falt.shape)\n",
    "        self.d1 = self.dense_layer(self.flat, 256, 'dense1')\n",
    "        \n",
    "        self.logits_v = tf.squeeze(self.dense_layer(self.d1, 1, 'logits_v'))\n",
    "        self.cost_v = 0.5 * tf.reduce_sum(tf.square(self.y_r - self.logits_v), axis=0) # mse\n",
    "        \n",
    "        self.logits_p = self.dense_layer(self.d1, self.num_actions, 'logits_p')\n",
    "        if Config.USE_LOG_SOFTMAX:\n",
    "            self.softmax_p = tf.nn.softmax(self.logits_p)\n",
    "            self.log_softmax_p = tf.nn.log_softmax(self.logits_p)\n",
    "            self.log_selected_action_prob = tf.reduce_sum(self.log_softmax_p * self.action_index, axis = 1)\n",
    "            \n",
    "            self.cost_p_1 = self.log_selected_action_prob * (self.y_r - tf.stop_gradient(self.logits_v))\n",
    "            self.cost_p_2 = -1 * self.var_beta * tf.reduce_sum(self.log_softmax_p*self.softmax_p, axis = 1)\n",
    "    \n",
    "        else:\n",
    "            self.softmax_p = (tf.nn.softmax(self.logits_p)+Config.MIN_POLICY)/(1.0+Config.MIN_POLICY * self.num_actions)\n",
    "            print('softmax_p : ',self.softmax_p)\n",
    "            self.selected_action_prob = tf.reduce_sum(self.softmax_p * slef.action_index, axis = 1)\n",
    "            self.cost_p_1 = tf.log(tf.maximum(self.selected_action_probe, self.log_epsilon))*(self.y_r - tf.stop_gradient(self.logits_v))\n",
    "            self.cost_p_2 = -1*self.var_beta*tf.reduce_sum(tf.log(tf.maximum(self.softmax_p, self.log_epsilon))*self.softmax_p, axis=1)\n",
    "            \n",
    "        self.cost_p_1_agg = tf.reduce_sum(self.cost_p_1, axis = 0)\n",
    "        self.cost_p_2_agg = tf.reduce_sum(self.cost_p_2, axis = 0)\n",
    "        self.cost_p = -(self.cost_p_1_agg+self.cost_p_2_agg)\n",
    "        \n",
    "        if Config.DUAL_RMSPROP:\n",
    "            self.opt_p = tf.train.RMSPropOptimizer(\n",
    "                learning_rate=self.var_learning_rate,\n",
    "                decay=Config.RMSPROP_DECAY,\n",
    "                momentum=Config.RMSPROP_MOMENTUM,\n",
    "                epsilon=Config.RMSPROP_EPSILON)\n",
    "            \n",
    "            self.opt_v = tf.train.RMSPropOptimizer(\n",
    "                learning_rate=self.var_learning_rate,\n",
    "                decay=Config.RMSPROP_DECAY,\n",
    "                momentum=Config.RMSPROP_MOMENTUM,\n",
    "                epsilon=Config.RMSPROP_EPSILON)\n",
    "        else:\n",
    "            self.cost_all = self.cost_p + self.cost_v\n",
    "            self.opt = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=self.var_learning_rate,\n",
    "                decay=Config.RMSPROP_DECAY,\n",
    "                momentum=Config.RMSPROP_MOMENTUM,\n",
    "                epsilon=Config.RMSPROP_EPSILON)\n",
    "            \n",
    "        if Config.USE_GRAD_CLIP:\n",
    "            if Config.DUAL_RMSPROP:\n",
    "                self.opt_grad_v = self.opt_v.compute_gradients(self.cost_v)\n",
    "                self.opt_grad_v_clipped = [(tf.clip_by_norm(g, Config.GRAD_CLIP_NORM),v)\n",
    "                                            for g,v in self.opt_grad_v if not g is None]\n",
    "                self.train_op_v = self.opt_v.apply_gradients(self.opt_grad_v_clipped)\n",
    "                \n",
    "                self.opt_grad_p = self.opt_p.compute_gradients(self.cost_p)\n",
    "                self.opt_grad_p_clipped = [(tf.clip_by_norm(g, Config.GRAD_CLIP_NORM), v)\n",
    "                                            for g,v in self.opt_grad_p if not g is None]\n",
    "                self.train_op_p = self.opt_p.apply_gradients(self.opt_grad_p_clipped)\n",
    "                self.train_op = [self.train_op_p, slef.train_op_v]\n",
    "                \n",
    "                self.opt_grad = self.opt.compute_gradients(self.cost_all)\n",
    "                self.opt_grad_clipped = [(tf.clip_by_average_norm(g, Config.GRAD_CLIP_NORM),v) for g, v in self.opt_grad]\n",
    "                self.train_op = self.opt.apply_gradients(self.opt_grad_clipped)\n",
    "            else:\n",
    "                if Config.DUAL_RMSPROP:\n",
    "                    self.train_op_p = self.opt_p.minimize(self.cost_v, global_step=self.global_step)\n",
    "                    self.train_op_v = self.opt_v.minimize(self.cost_p, global_step=self.global_step)\n",
    "                    self.train_op = [self.train_op_p, self.train_op_v]\n",
    "                    \n",
    "                else:\n",
    "                    self.train_op = self.opt.minimize(self.cost_all, global_step=self.global_step)\n",
    "                    \n",
    "    def _create_tensor_board(self):\n",
    "        summaries = tf.get_collection(tf.GraphKeys.SUMMARIES)\n",
    "        summaries.append(tf.summary.scalar(\"Pcost_advantage\", self.cost_p_1_agg))\n",
    "        summaries.append(tf.summary.scalar(\"Pcost_entropy\", self.cost_p_2_agg))\n",
    "        summaries.append(tf.summary.scalar(\"Pcost\", self.cost_p))\n",
    "        summaries.append(tf.summary.scalar(\"Vcost\", self.cost_v))\n",
    "        summaries.append(tf.summary.scalar(\"LearningRate\", self.var_learning_rate))\n",
    "        summaries.append(tf.summary.scalar(\"Beta\", self.var_beta))\n",
    "        for var in tf.trainable_variables():\n",
    "            summaries.append(tf.summary.histogram(\"weights_%s\" % var.name, var))\n",
    "\n",
    "        summaries.append(tf.summary.histogram(\"activation_n1\", self.n1))\n",
    "        summaries.append(tf.summary.histogram(\"activation_n2\", self.n2))\n",
    "        summaries.append(tf.summary.histogram(\"activation_d2\", self.d1))\n",
    "        summaries.append(tf.summary.histogram(\"activation_v\", self.logits_v))\n",
    "        summaries.append(tf.summary.histogram(\"activation_p\", self.softmax_p))\n",
    "\n",
    "        self.summary_op = tf.summary.merge(summaries)\n",
    "        self.log_writer = tf.summary.FileWriter(\"logs/%s\" % self.model_name, self.sess.graph)\n",
    "\n",
    "    def dense_layer(self, input, out_dim, name, func=tf.nn.relu):\n",
    "        in_dim = input.get_shape().as_list()[-1]\n",
    "        d = 1.0 / np.sqrt(in_dim)\n",
    "        with tf.variable_scope(name):\n",
    "            w_init = tf.random_uniform_initializer(-d, d)\n",
    "            b_init = tf.random_uniform_initializer(-d, d)\n",
    "            w = tf.get_variable('w', dtype=tf.float32, shape=[in_dim, out_dim], initializer=w_init)\n",
    "            b = tf.get_variable('b', shape=[out_dim], initializer=b_init)\n",
    "            \n",
    "            output = tf.matmul(input, w) + b\n",
    "            if func is not None:\n",
    "                output = func(output)\n",
    "                \n",
    "            return output\n",
    "        \n",
    "    def conv2d_layer(self, input, filter_size, out_dim, name, strides, func=tf.nn.relu):\n",
    "        in_dim = input.get_shape().as_list()[-1]\n",
    "        d = 1.0 / np.sqrt(filter_size * filter_size * in_dim)\n",
    "        with tf.variable_scope(name):\n",
    "            w_init = tf.random_uniform_initializer(-d, d)\n",
    "            b_init = tf.random_uniform_initializer(-d, d)\n",
    "            w = tf.get_variable('w', \n",
    "                                shape=[filter_size, filter_size, in_dim, out_dim],\n",
    "                                dtype=tf.float32,\n",
    "                                initializer=w_init)\n",
    "            b = tf.get_variable('b', shape=[out_dim], initializer=b_init)\n",
    "            \n",
    "            output = tf.nn.conv2d(input, w, strides=strides, padding='SAME') + b\n",
    "            if func is not None:\n",
    "                output = func(output)\n",
    "                \n",
    "            return output\n",
    "        \n",
    "    def __get_base_feed_dict(self):\n",
    "        return {self.var_beta: self.beta, self.var_learning_rate: self.learning_rate}\n",
    "    \n",
    "    def get_global_step(self):\n",
    "        step = self.sess.run(self.global_step)\n",
    "        return step\n",
    "    \n",
    "    def predict_single(self, x):\n",
    "        return self.predict_p(x[None, :])[0]\n",
    "    \n",
    "    def predict_v(self, x):\n",
    "        prediction = self.sess.run(self.logits_v, feed_dict={self.x:x})\n",
    "        return prediction\n",
    "    \n",
    "    def predcit_p(self, x):\n",
    "        return self.sess.run([self.softmax_p, self.logits_v], feed_dict={self.x:x})\n",
    "    \n",
    "       \n",
    "    def train(self, x, y_r, a, trainer_id):\n",
    "        feed_dict = self.__get_base_feed_dict()\n",
    "        feed_dict.update({self.x: x, self.y_r: y_r, self.action_index: a})\n",
    "        self.sess.run(self.train_op, feed_dict=feed_dict)\n",
    "\n",
    "    def log(self, x, y_r, a):\n",
    "        feed_dict = self.__get_base_feed_dict()\n",
    "        feed_dict.update({self.x: x, self.y_r: y_r, self.action_index: a})\n",
    "        step, summary = self.sess.run([self.global_step, self.summary_op], feed_dict=feed_dict)\n",
    "        self.log_writer.add_summary(summary, step)\n",
    "\n",
    "    def _checkpoint_filename(self, episode):\n",
    "        return 'checkpoints/%s_%08d' % (self.model_name, episode)\n",
    "    \n",
    "    def _get_episode_from_filename(self, filename):\n",
    "        # TODO: hacky way of getting the episode. ideally episode should be stored as a TF variable\n",
    "        return int(re.split('/|_|\\.', filename)[2])\n",
    "\n",
    "    def save(self, episode):\n",
    "        self.saver.save(self.sess, self._checkpoint_filename(episode))\n",
    "\n",
    "    def load(self):\n",
    "        filename = tf.train.latest_checkpoint(os.path.dirname(self._checkpoint_filename(episode=0)))\n",
    "        if Config.LOAD_EPISODE > 0:\n",
    "            filename = self._checkpoint_filename(Config.LOAD_EPISODE)\n",
    "        self.saver.restore(self.sess, filename)\n",
    "        return self._get_episode_from_filename(filename)\n",
    "       \n",
    "    def get_variables_names(self):\n",
    "        return [var.name for var in self.graph.get_collection('trainable_variables')]\n",
    "\n",
    "    def get_variable_value(self, name):\n",
    "        return self.sess.run(self.graph.get_tensor_by_name(name))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ga3c",
   "language": "python",
   "name": "ga3c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
